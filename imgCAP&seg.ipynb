{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0NLDk-o-phAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ASd7hj-phBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "0L6B8LEkpiwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyE_8W0kqKcz",
        "outputId": "781681aa-d2d4-4a49-f07e-42516e0f0da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from datasets import load_dataset\n",
        "\n",
        "class COCODataset(Dataset):\n",
        "    def __init__(self, vocab, transform=None, max_caption_length=20, split=\"train[:1%]\"):\n",
        "        \"\"\"\n",
        "        COCO Dataset using HuggingFace for captioning\n",
        "        \"\"\"\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        self.max_caption_length = max_caption_length\n",
        "\n",
        "        # Load COCO via Hugging Face (lazy loading)\n",
        "        self.dataset = load_dataset(\"HuggingFaceM4/COCO\", split=split)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        image_url = sample['image']['url']\n",
        "        caption = self.preprocess_caption(sample['caption'])\n",
        "\n",
        "        # Load image from URL\n",
        "        try:\n",
        "            response = requests.get(image_url)\n",
        "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        except:\n",
        "            image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Process caption\n",
        "        caption_tokens = self.caption_to_tokens(caption)\n",
        "\n",
        "        return image, caption_tokens, idx  # using idx as image_id here\n",
        "\n",
        "    def preprocess_caption(self, caption):\n",
        "        \"\"\"Clean and preprocess caption text\"\"\"\n",
        "        caption = caption.lower()\n",
        "        caption = re.sub(r'[^\\w\\s]', '', caption)\n",
        "        return caption.strip()\n",
        "\n",
        "    def caption_to_tokens(self, caption):\n",
        "        \"\"\"Convert caption to token indices\"\"\"\n",
        "        tokens = word_tokenize(caption)\n",
        "        tokens = ['<start>'] + tokens + ['<end>']\n",
        "\n",
        "        if len(tokens) > self.max_caption_length:\n",
        "            tokens = tokens[:self.max_caption_length - 1] + ['<end>']\n",
        "        else:\n",
        "            tokens.append('<end>')\n",
        "            tokens.extend(['<pad>'] * (self.max_caption_length - len(tokens)))\n",
        "\n",
        "        token_indices = [self.vocab.get(token, self.vocab.get('<unk>', 0)) for token in tokens]\n",
        "        return torch.tensor(token_indices)\n"
      ],
      "metadata": {
        "id": "mosr0lVNqOUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VocabularyBuilder:\n",
        "    def __init__(self, min_freq=2):\n",
        "        self.word2idx = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n",
        "        self.idx2word = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n",
        "        self.min_freq = min_freq\n",
        "        self.word_freq = defaultdict(int)\n",
        "\n",
        "    def build_vocab(self, captions):\n",
        "        \"\"\"Build vocabulary from captions\"\"\"\n",
        "        # Count word frequencies\n",
        "        for caption in captions:\n",
        "            words = word_tokenize(caption.lower())\n",
        "            for word in words:\n",
        "                self.word_freq[word] += 1\n",
        "\n",
        "        # Add words that meet minimum frequency\n",
        "        idx = len(self.word2idx)\n",
        "        for word, freq in self.word_freq.items():\n",
        "            if freq >= self.min_freq and word not in self.word2idx:\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
        "        return self.word2idx, self.idx2word\n",
        "\n",
        "# Mock COCO data creation (for demonstration)\n",
        "def create_mock_coco_data():\n",
        "    \"\"\"Create mock COCO data structure for demonstration\"\"\"\n",
        "    mock_data = {\n",
        "        'images': [\n",
        "            {'id': 1, 'file_name': 'image1.jpg', 'height': 224, 'width': 224},\n",
        "            {'id': 2, 'file_name': 'image2.jpg', 'height': 224, 'width': 224},\n",
        "        ],\n",
        "        'annotations': [\n",
        "            {'id': 1, 'image_id': 1, 'caption': 'A cat sitting on a chair'},\n",
        "            {'id': 2, 'image_id': 1, 'caption': 'Orange cat relaxing indoors'},\n",
        "            {'id': 3, 'image_id': 2, 'caption': 'A dog running in the park'},\n",
        "            {'id': 4, 'image_id': 2, 'caption': 'Happy golden retriever playing outside'},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Save mock data\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    with open('data/mock_coco.json', 'w') as f:\n",
        "        json.dump(mock_data, f)\n",
        "\n",
        "    return mock_data\n",
        "\n",
        "# Create mock data\n",
        "mock_data = create_mock_coco_data()\n",
        "print(\"Mock COCO data created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhMLvBCxqoKo",
        "outputId": "afa77469-aa57-40ec-9fec-5cdc8059cf4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mock COCO data created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, embed_size=256):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        # Use pre-trained ResNet\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        # Remove last layer\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "        return features"
      ],
      "metadata": {
        "id": "NrjrQ0oXqs8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, dropout=0.5):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.attention = nn.Linear(decoder_dim, attention_dim)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim, decoder_dim, bias=True)\n",
        "        self.init_h = nn.Linear(attention_dim, decoder_dim)\n",
        "        self.init_c = nn.Linear(attention_dim, decoder_dim)\n",
        "        self.f_beta = nn.Linear(decoder_dim, attention_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        batch_size = encoder_out.size(0)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Sort input data by caption lengths\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h = self.init_h(encoder_out)\n",
        "        c = self.init_c(encoder_out)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word prediction scores\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size)\n",
        "\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "\n",
        "            # Attention mechanism (simplified)\n",
        "            attention_weighted_encoding = encoder_out[:batch_size_t]\n",
        "\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t])\n",
        "            )\n",
        "\n",
        "            preds = self.fc(self.dropout(h))\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, sort_ind\n"
      ],
      "metadata": {
        "id": "yZ4IJybcq3Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptioningModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1):\n",
        "        super(CaptioningModel, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # CNN Encoder\n",
        "        self.encoder = CNNEncoder(embed_size)\n",
        "\n",
        "        # RNN Decoder\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        # Encode images\n",
        "        features = self.encoder(images)\n",
        "\n",
        "        # Prepare captions (remove last token for input)\n",
        "        captions_input = captions[:, :-1]\n",
        "        embeddings = self.embedding(captions_input)\n",
        "\n",
        "        # Concatenate image features and caption embeddings\n",
        "        features = features.unsqueeze(1)\n",
        "        inputs = torch.cat((features, embeddings), dim=1)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.linear(hiddens)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate_caption(self, image, vocab, max_length=20):\n",
        "        \"\"\"Generate caption for a single image\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode image\n",
        "            features = self.encoder(image.unsqueeze(0))\n",
        "\n",
        "            # Start with <start> token\n",
        "            inputs = torch.tensor([vocab['<start>']]).unsqueeze(0)\n",
        "            hidden = None\n",
        "            caption = []\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                # Embed input\n",
        "                embedded = self.embedding(inputs)\n",
        "\n",
        "                if hidden is None:\n",
        "                    # First step: use image features\n",
        "                    lstm_input = torch.cat([features.unsqueeze(1), embedded], dim=1)\n",
        "                else:\n",
        "                    lstm_input = embedded\n",
        "\n",
        "                # LSTM forward\n",
        "                output, hidden = self.lstm(lstm_input, hidden)\n",
        "\n",
        "                # Get prediction\n",
        "                pred = self.linear(output[:, -1, :])\n",
        "                predicted_id = pred.argmax(dim=1)\n",
        "\n",
        "                # Add to caption\n",
        "                caption.append(predicted_id.item())\n",
        "\n",
        "                # Stop if <end> token\n",
        "                if predicted_id.item() == vocab.get('<end>', 2):\n",
        "                    break\n",
        "\n",
        "                # Update input for next iteration\n",
        "                inputs = predicted_id.unsqueeze(0)\n",
        "\n",
        "            return caption"
      ],
      "metadata": {
        "id": "4lzbeAnYq7HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels=3, n_classes=21):  # 21 classes for Pascal VOC\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder1 = self.conv_block(n_channels, 64)\n",
        "        self.encoder2 = self.conv_block(64, 128)\n",
        "        self.encoder3 = self.conv_block(128, 256)\n",
        "        self.encoder4 = self.conv_block(256, 512)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.decoder4 = self.conv_block(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.decoder3 = self.conv_block(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.decoder2 = self.conv_block(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.decoder1 = self.conv_block(128, 64)\n",
        "\n",
        "        # Final layer\n",
        "        self.final_conv = nn.Conv2d(64, n_classes, 1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(F.max_pool2d(enc1, 2))\n",
        "        enc3 = self.encoder3(F.max_pool2d(enc2, 2))\n",
        "        enc4 = self.encoder4(F.max_pool2d(enc3, 2))\n",
        "\n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n",
        "\n",
        "        # Decoder\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "\n",
        "        return self.final_conv(dec1)"
      ],
      "metadata": {
        "id": "wU94D_Q8rGXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskRCNNHead(nn.Module):\n",
        "    def __init__(self, in_channels=256, num_classes=80):\n",
        "        super(MaskRCNNHead, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 256, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.deconv = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
        "        self.predictor = nn.Conv2d(256, num_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.deconv(x))\n",
        "        return self.predictor(x)"
      ],
      "metadata": {
        "id": "GG2lMn-jrUUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IntegratedModel(nn.Module):\n",
        "    def __init__(self, vocab_size, num_seg_classes=21):\n",
        "        super(IntegratedModel, self).__init__()\n",
        "\n",
        "        # Shared CNN backbone\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Remove avgpool and fc\n",
        "\n",
        "        # Caption branch\n",
        "        self.caption_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.caption_fc = nn.Linear(2048, 512)\n",
        "        self.caption_embedding = nn.Embedding(vocab_size, 256)\n",
        "        self.caption_lstm = nn.LSTM(256, 512, batch_first=True)\n",
        "        self.caption_output = nn.Linear(512, vocab_size)\n",
        "\n",
        "        # Segmentation branch\n",
        "        self.seg_decoder = nn.Sequential(\n",
        "            nn.Conv2d(2048, 512, 3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(512, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(256, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(128, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(64, num_seg_classes, 3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, captions=None, mode='both'):\n",
        "        # Extract shared features\n",
        "        shared_features = self.backbone(images)\n",
        "\n",
        "        outputs = {}\n",
        "\n",
        "        if mode in ['caption', 'both'] and captions is not None:\n",
        "            # Caption branch\n",
        "            caption_features = self.caption_pool(shared_features).flatten(1)\n",
        "            caption_features = self.caption_fc(caption_features)\n",
        "\n",
        "            # Process captions\n",
        "            caption_embeddings = self.caption_embedding(captions[:, :-1])\n",
        "\n",
        "            # Add image features to beginning\n",
        "            caption_features = caption_features.unsqueeze(1)\n",
        "            lstm_input = torch.cat([caption_features, caption_embeddings], dim=1)\n",
        "\n",
        "            lstm_out, _ = self.caption_lstm(lstm_input)\n",
        "            caption_logits = self.caption_output(lstm_out)\n",
        "            outputs['captions'] = caption_logits\n",
        "\n",
        "        if mode in ['segment', 'both']:\n",
        "            # Segmentation branch\n",
        "            seg_logits = self.seg_decoder(shared_features)\n",
        "            outputs['segmentation'] = seg_logits\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate_caption(self, image, vocab, max_length=20):\n",
        "        \"\"\"Generate caption for inference\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            shared_features = self.backbone(image.unsqueeze(0))\n",
        "            caption_features = self.caption_pool(shared_features).flatten(1)\n",
        "            caption_features = self.caption_fc(caption_features)\n",
        "\n",
        "            # Generate caption word by word\n",
        "            caption = []\n",
        "            hidden = None\n",
        "            input_word = torch.tensor([vocab['<start>']]).unsqueeze(0)\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                word_embed = self.caption_embedding(input_word)\n",
        "\n",
        "                if hidden is None:\n",
        "                    lstm_input = torch.cat([caption_features.unsqueeze(1), word_embed], dim=1)\n",
        "                else:\n",
        "                    lstm_input = word_embed\n",
        "\n",
        "                output, hidden = self.caption_lstm(lstm_input, hidden)\n",
        "                word_logits = self.caption_output(output[:, -1, :])\n",
        "                predicted_word = word_logits.argmax(dim=1)\n",
        "\n",
        "                caption.append(predicted_word.item())\n",
        "\n",
        "                if predicted_word.item() == vocab.get('<end>', 2):\n",
        "                    break\n",
        "\n",
        "                input_word = predicted_word.unsqueeze(0)\n",
        "\n",
        "            return caption\n"
      ],
      "metadata": {
        "id": "kmNFrfdPrWR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_integrated_model(model, train_loader, criterion_caption, criterion_seg, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (images, captions, seg_masks) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "        seg_masks = seg_masks.to(device) if seg_masks is not None else None\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, captions, mode='both')\n",
        "\n",
        "        # Calculate losses\n",
        "        caption_loss = 0\n",
        "        seg_loss = 0\n",
        "\n",
        "        if 'captions' in outputs:\n",
        "            caption_targets = captions[:, 1:]  # Remove <start> token\n",
        "            caption_loss = criterion_caption(\n",
        "                outputs['captions'].reshape(-1, outputs['captions'].size(-1)),\n",
        "                caption_targets.reshape(-1)\n",
        "            )\n",
        "\n",
        "        if 'segmentation' in outputs and seg_masks is not None:\n",
        "            seg_loss = criterion_seg(outputs['segmentation'], seg_masks)\n",
        "\n",
        "        # Combined loss\n",
        "        total_batch_loss = caption_loss + seg_loss\n",
        "        total_batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += total_batch_loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Batch {batch_idx}, Caption Loss: {caption_loss:.4f}, Seg Loss: {seg_loss:.4f}')\n",
        "\n",
        "    return total_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "hqX0zbHhrczW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, vocab, device):\n",
        "    model.eval()\n",
        "    total_caption_loss = 0\n",
        "    total_seg_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions, seg_masks in val_loader:\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "            seg_masks = seg_masks.to(device) if seg_masks is not None else None\n",
        "\n",
        "            outputs = model(images, captions, mode='both')\n",
        "\n",
        "            # Calculate metrics (simplified)\n",
        "            if 'captions' in outputs:\n",
        "                caption_targets = captions[:, 1:]\n",
        "                caption_loss = F.cross_entropy(\n",
        "                    outputs['captions'].reshape(-1, outputs['captions'].size(-1)),\n",
        "                    caption_targets.reshape(-1)\n",
        "                )\n",
        "                total_caption_loss += caption_loss.item()\n",
        "\n",
        "            if 'segmentation' in outputs and seg_masks is not None:\n",
        "                seg_loss = F.cross_entropy(outputs['segmentation'], seg_masks)\n",
        "                total_seg_loss += seg_loss.item()\n",
        "\n",
        "    return total_caption_loss / len(val_loader), total_seg_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "L6AJ96LPriE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGzdXzJVslv1",
        "outputId": "0de580ef-8afb-476c-9c54-b42409e4aacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(model, image, vocab, idx2word, device, seg_classes=None):\n",
        "    \"\"\"Visualize captioning and segmentation results\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Generate caption\n",
        "    caption_ids = model.generate_caption(image, vocab)\n",
        "    caption_words = [idx2word.get(idx, '<unk>') for idx in caption_ids]\n",
        "    caption = ' '.join([word for word in caption_words if word not in ['<start>', '<end>', '<pad>']])\n",
        "\n",
        "    # Generate segmentation\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image.unsqueeze(0).to(device), mode='segment')\n",
        "        seg_pred = outputs['segmentation'].squeeze(0).cpu()\n",
        "        seg_mask = seg_pred.argmax(dim=0)\n",
        "\n",
        "    # Plot results\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Original image\n",
        "    img_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "    img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "    axes[0].imshow(img_np)\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Segmentation mask\n",
        "    axes[1].imshow(seg_mask, cmap='tab20')\n",
        "    axes[1].set_title('Segmentation Mask')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Caption\n",
        "    axes[2].text(0.5, 0.5, f'Caption: {caption}',\n",
        "                horizontalalignment='center', verticalalignment='center',\n",
        "                transform=axes[2].transAxes, fontsize=12, wrap=True)\n",
        "    axes[2].set_title('Generated Caption')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return caption, seg_mask\n",
        "\n",
        "# Example usage and setup\n",
        "def setup_training():\n",
        "    \"\"\"Setup training configuration\"\"\"\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create vocabulary (mock)\n",
        "    sample_captions = [\n",
        "        \"a cat sitting on a chair\",\n",
        "        \"a dog running in the park\",\n",
        "        \"people walking on the street\",\n",
        "        \"cars driving on the road\"\n",
        "    ]\n",
        "\n",
        "    vocab_builder = VocabularyBuilder()\n",
        "    word2idx, idx2word = vocab_builder.build_vocab(sample_captions)\n",
        "\n",
        "    # Model configuration\n",
        "    vocab_size = len(word2idx)\n",
        "    embed_size = 256\n",
        "    hidden_size = 512\n",
        "    num_seg_classes = 21\n",
        "\n",
        "    # Initialize models\n",
        "    captioning_model = CaptioningModel(vocab_size, embed_size, hidden_size)\n",
        "    segmentation_model = UNet(n_channels=3, n_classes=num_seg_classes)\n",
        "    integrated_model = IntegratedModel(vocab_size, num_seg_classes)\n",
        "\n",
        "    # Move to device\n",
        "    captioning_model.to(device)\n",
        "    segmentation_model.to(device)\n",
        "    integrated_model.to(device)\n",
        "\n",
        "    print(\"Models initialized and moved to device!\")\n",
        "\n",
        "    return {\n",
        "        'device': device,\n",
        "        'vocab': word2idx,\n",
        "        'idx2word': idx2word,\n",
        "        'captioning_model': captioning_model,\n",
        "        'segmentation_model': segmentation_model,\n",
        "        'integrated_model': integrated_model\n",
        "    }\n",
        "setup_dict = setup_training()\n",
        "print(\"\\nSetup complete! You can now:\")\n",
        "print(\"1. Load your COCO dataset\")\n",
        "print(\"2. Create data loaders\")\n",
        "print(\"3. Start training with the integrated model\")\n",
        "print(\"4. Evaluate on validation data\")\n",
        "print(\"5. Visualize results\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbpUd5HUrmMd",
        "outputId": "a3f7954a-ed10-4dca-8723-d9376fbd4f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocabulary size: 7\n",
            "Models initialized and moved to device!\n",
            "\n",
            "Setup complete! You can now:\n",
            "1. Load your COCO dataset\n",
            "2. Create data loaders\n",
            "3. Start training with the integrated model\n",
            "4. Evaluate on validation data\n",
            "5. Visualize results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def example_training_setup():\n",
        "    \"\"\"Example of how to set up the training loop\"\"\"\n",
        "\n",
        "    # Hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    batch_size = 32\n",
        "    num_epochs = 10\n",
        "\n",
        "    device = setup_dict['device']\n",
        "    model = setup_dict['integrated_model']\n",
        "    vocab = setup_dict['vocab']\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_caption = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
        "    criterion_seg = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(f\"Training setup complete!\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Number of epochs: {num_epochs}\")\n",
        "\n",
        "    return {\n",
        "        'optimizer': optimizer,\n",
        "        'criterion_caption': criterion_caption,\n",
        "        'criterion_seg': criterion_seg,\n",
        "        'batch_size': batch_size,\n",
        "        'num_epochs': num_epochs\n",
        "    }\n",
        "\n",
        "training_setup = example_training_setup()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"IMPLEMENTATION COMPLETE!\")\n",
        "print(\"=\"*50)\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Replace mock data with real COCO dataset\")\n",
        "print(\"2. Implement proper data loaders\")\n",
        "print(\"3. Run training loop\")\n",
        "print(\"4. Add evaluation metrics (BLEU, IoU, etc.)\")\n",
        "print(\"5. Implement model checkpointing\")\n",
        "print(\"6. Add tensorboard logging\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKRpVo_os_HH",
        "outputId": "5721509b-d12d-4d93-d788-364296282269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training setup complete!\n",
            "Learning rate: 0.001\n",
            "Batch size: 32\n",
            "Number of epochs: 10\n",
            "\n",
            "==================================================\n",
            "IMPLEMENTATION COMPLETE!\n",
            "==================================================\n",
            "Next steps:\n",
            "1. Replace mock data with real COCO dataset\n",
            "2. Implement proper data loaders\n",
            "3. Run training loop\n",
            "4. Add evaluation metrics (BLEU, IoU, etc.)\n",
            "5. Implement model checkpointing\n",
            "6. Add tensorboard logging\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RealCOCODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, ann_file, vocab, transform=None, max_caption_length=20):\n",
        "        \"\"\"\n",
        "        Real COCO Dataset implementation\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.max_caption_length = max_caption_length\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # Initialize COCO API\n",
        "        self.coco = COCO(ann_file)\n",
        "        self.img_ids = list(self.coco.imgs.keys())\n",
        "\n",
        "        # Filter images that have both captions and segmentation annotations\n",
        "        self.valid_img_ids = []\n",
        "        for img_id in self.img_ids:\n",
        "            ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
        "            anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "            # Check if image has both captions and segmentation\n",
        "            has_caption = any('caption' in ann for ann in anns)\n",
        "            has_segmentation = any('segmentation' in ann for ann in anns)\n",
        "\n",
        "            if has_caption and has_segmentation:\n",
        "                self.valid_img_ids.append(img_id)\n",
        "\n",
        "        print(f\"Found {len(self.valid_img_ids)} images with both captions and segmentation\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.valid_img_ids[idx]\n",
        "\n",
        "        # Load image\n",
        "        img_info = self.coco.loadImgs(img_id)[0]\n",
        "        image_path = os.path.join(self.root_dir, img_info['file_name'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Get annotations\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
        "        anns = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Get caption\n",
        "        captions = [ann['caption'] for ann in anns if 'caption' in ann]\n",
        "        if captions:\n",
        "            caption = random.choice(captions)  # Random caption if multiple\n",
        "        else:\n",
        "            caption = \"no caption available\"\n",
        "\n",
        "        # Create segmentation mask\n",
        "        seg_mask = self.create_segmentation_mask(anns, img_info['height'], img_info['width'])\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            # Also transform segmentation mask\n",
        "            seg_mask = torch.from_numpy(seg_mask).long()\n",
        "\n",
        "        # Process caption\n",
        "        caption_tokens = self.caption_to_tokens(caption)\n",
        "\n",
        "        return image, caption_tokens, seg_mask\n",
        "\n",
        "    def create_segmentation_mask(self, anns, height, width):\n",
        "        \"\"\"Create segmentation mask from COCO annotations\"\"\"\n",
        "        mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "        for ann in anns:\n",
        "            if 'segmentation' in ann:\n",
        "                category_id = ann['category_id']\n",
        "                if isinstance(ann['segmentation'], list):\n",
        "                    # Polygon format\n",
        "                    for seg in ann['segmentation']:\n",
        "                        poly = np.array(seg).reshape(-1, 2)\n",
        "                        cv2.fillPoly(mask, [poly.astype(np.int32)], category_id)\n",
        "                else:\n",
        "                    # RLE format\n",
        "                    rle = coco_mask.frPyObjects(ann['segmentation'], height, width)\n",
        "                    m = coco_mask.decode(rle)\n",
        "                    mask[m > 0] = category_id\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def caption_to_tokens(self, caption):\n",
        "        \"\"\"Convert caption to token indices\"\"\"\n",
        "        caption = caption.lower().strip()\n",
        "        tokens = caption.split()\n",
        "        tokens = ['<start>'] + tokens + ['<end>']\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(tokens) > self.max_caption_length:\n",
        "            tokens = tokens[:self.max_caption_length]\n",
        "        else:\n",
        "            tokens.extend(['<pad>'] * (self.max_caption_length - len(tokens)))\n",
        "\n",
        "        # Convert to indices\n",
        "        token_indices = [self.vocab.get(token, self.vocab.get('<unk>', 0)) for token in tokens]\n",
        "        return torch.tensor(token_indices)\n"
      ],
      "metadata": {
        "id": "3EUSqzA8BZ9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Loading Setup\n"
      ],
      "metadata": {
        "id": "MlA5pnxyCItc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loaders(vocab, batch_size=32):\n",
        "    \"\"\"Create train and validation data loaders\"\"\"\n",
        "\n",
        "    # Define transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Create datasets (you'll need to download COCO dataset first)\n",
        "    # train_dataset = RealCOCODataset(\n",
        "    #     root_dir='path/to/coco/train2017',\n",
        "    #     ann_file='path/to/coco/annotations/instances_train2017.json',\n",
        "    #     vocab=vocab,\n",
        "    #     transform=train_transform\n",
        "    # )\n",
        "\n",
        "    # val_dataset = RealCOCODataset(\n",
        "    #     root_dir='path/to/coco/val2017',\n",
        "    #     ann_file='path/to/coco/annotations/instances_val2017.json',\n",
        "    #     vocab=vocab,\n",
        "    #     transform=val_transform\n",
        "    # )\n",
        "\n",
        "    # For now, create dummy datasets for demonstration\n",
        "    class DummyDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, size=1000, vocab_size=len(vocab)):\n",
        "            self.size = size\n",
        "            self.vocab_size = vocab_size\n",
        "\n",
        "        def __len__(self):\n",
        "            return self.size\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Dummy data\n",
        "            image = torch.randn(3, 224, 224)\n",
        "            caption = torch.randint(0, self.vocab_size, (20,))\n",
        "            seg_mask = torch.randint(0, 21, (224, 224))\n",
        "            return image, caption, seg_mask\n",
        "\n",
        "    train_dataset = DummyDataset(1000)\n",
        "    val_dataset = DummyDataset(200)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "JW904b_mBdo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Enhanced Training Loop with Logging\n"
      ],
      "metadata": {
        "id": "mZWdRcFMCPlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_logging(model, train_loader, val_loader, vocab, idx2word,\n",
        "                            num_epochs=10, device='cuda'):\n",
        "    \"\"\"Complete training loop with logging and checkpointing\"\"\"\n",
        "\n",
        "    # Setup\n",
        "    model.to(device)\n",
        "    criterion_caption = torch.nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
        "    criterion_seg = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_caption_loss = 0.0\n",
        "        train_seg_loss = 0.0\n",
        "\n",
        "        for batch_idx, (images, captions, seg_masks) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "            seg_masks = seg_masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images, captions, mode='both')\n",
        "\n",
        "            # Calculate losses\n",
        "            caption_targets = captions[:, 1:]  # Remove <start> token\n",
        "            caption_loss = criterion_caption(\n",
        "                outputs['captions'].reshape(-1, outputs['captions'].size(-1)),\n",
        "                caption_targets.reshape(-1)\n",
        "            )\n",
        "\n",
        "            seg_loss = criterion_seg(outputs['segmentation'], seg_masks)\n",
        "\n",
        "            # Combined loss (you can adjust weights)\n",
        "            total_loss = caption_loss + 0.5 * seg_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate losses\n",
        "            train_loss += total_loss.item()\n",
        "            train_caption_loss += caption_loss.item()\n",
        "            train_seg_loss += seg_loss.item()\n",
        "\n",
        "            # Print progress\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Batch {batch_idx}/{len(train_loader)}, '\n",
        "                      f'Loss: {total_loss.item():.4f}, '\n",
        "                      f'Caption: {caption_loss.item():.4f}, '\n",
        "                      f'Seg: {seg_loss.item():.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_caption_loss = 0.0\n",
        "        val_seg_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, captions, seg_masks in val_loader:\n",
        "                images = images.to(device)\n",
        "                captions = captions.to(device)\n",
        "                seg_masks = seg_masks.to(device)\n",
        "\n",
        "                outputs = model(images, captions, mode='both')\n",
        "\n",
        "                caption_targets = captions[:, 1:]\n",
        "                caption_loss = criterion_caption(\n",
        "                    outputs['captions'].reshape(-1, outputs['captions'].size(-1)),\n",
        "                    caption_targets.reshape(-1)\n",
        "                )\n",
        "\n",
        "                seg_loss = criterion_seg(outputs['segmentation'], seg_masks)\n",
        "                total_loss = caption_loss + 0.5 * seg_loss\n",
        "\n",
        "                val_loss += total_loss.item()\n",
        "                val_caption_loss += caption_loss.item()\n",
        "                val_seg_loss += seg_loss.item()\n",
        "\n",
        "        # Calculate average losses\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_train_caption = train_caption_loss / len(train_loader)\n",
        "        avg_val_caption = val_caption_loss / len(val_loader)\n",
        "        avg_train_seg = train_seg_loss / len(train_loader)\n",
        "        avg_val_seg = val_seg_loss / len(val_loader)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1} Results:')\n",
        "        print(f'Train Loss: {avg_train_loss:.4f} (Caption: {avg_train_caption:.4f}, Seg: {avg_train_seg:.4f})')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f} (Caption: {avg_val_caption:.4f}, Seg: {avg_val_seg:.4f})')\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_val_loss,\n",
        "                'vocab': vocab,\n",
        "                'idx2word': idx2word\n",
        "            }, 'best_model.pth')\n",
        "            print(f'Best model saved with validation loss: {best_val_loss:.4f}')\n",
        "\n",
        "        # Step scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Generate sample results every 2 epochs\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            generate_sample_results(model, val_loader, vocab, idx2word, device)\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "OxRdeNdJBlan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Evaluation Metrics\n"
      ],
      "metadata": {
        "id": "62OOrdImCTic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(model, test_loader, vocab, idx2word, device):\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # For caption evaluation (simplified BLEU)\n",
        "    from nltk.translate.bleu_score import sentence_bleu\n",
        "    bleu_scores = []\n",
        "\n",
        "    # For segmentation evaluation\n",
        "    intersection = 0\n",
        "    union = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions, seg_masks in test_loader:\n",
        "            images = images.to(device)\n",
        "            seg_masks = seg_masks.to(device)\n",
        "\n",
        "            # Generate captions\n",
        "            for i in range(images.size(0)):\n",
        "                generated_caption_ids = model.generate_caption(images[i], vocab)\n",
        "                generated_words = [idx2word[idx] for idx in generated_caption_ids\n",
        "                                 if idx in idx2word and idx2word[idx] not in ['<start>', '<end>', '<pad>']]\n",
        "\n",
        "                # Get ground truth caption\n",
        "                gt_caption_ids = captions[i].cpu().numpy()\n",
        "                gt_words = [idx2word[idx] for idx in gt_caption_ids\n",
        "                           if idx in idx2word and idx2word[idx] not in ['<start>', '<end>', '<pad>']]\n",
        "\n",
        "                # Calculate BLEU score\n",
        "                if gt_words:\n",
        "                    bleu = sentence_bleu([gt_words], generated_words)\n",
        "                    bleu_scores.append(bleu)\n",
        "\n",
        "            # Segmentation evaluation\n",
        "            outputs = model(images, mode='segment')\n",
        "            pred_masks = outputs['segmentation'].argmax(dim=1)\n",
        "\n",
        "            # Calculate IoU\n",
        "            for i in range(pred_masks.size(0)):\n",
        "                pred = pred_masks[i].cpu()\n",
        "                gt = seg_masks[i].cpu()\n",
        "\n",
        "                intersection += (pred * gt).sum().item()\n",
        "                union += (pred + gt).clamp(0, 1).sum().item()\n",
        "\n",
        "    # Calculate final metrics\n",
        "    avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n",
        "    iou = intersection / union if union > 0 else 0\n",
        "\n",
        "    print(f\"Evaluation Results:\")\n",
        "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
        "    print(f\"IoU Score: {iou:.4f}\")\n",
        "\n",
        "    return avg_bleu, iou"
      ],
      "metadata": {
        "id": "J8nrvmEeBplT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Sample Generation Function\n"
      ],
      "metadata": {
        "id": "O156azGWCYAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample_results(model, data_loader, vocab, idx2word, device, num_samples=3):\n",
        "    \"\"\"Generate and display sample results\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (images, captions, seg_masks) in enumerate(data_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            image = images[0].to(device)\n",
        "\n",
        "            # Generate caption\n",
        "            caption_ids = model.generate_caption(image, vocab)\n",
        "            caption_words = [idx2word.get(idx, '<unk>') for idx in caption_ids]\n",
        "            generated_caption = ' '.join([w for w in caption_words if w not in ['<start>', '<end>', '<pad>']])\n",
        "\n",
        "            # Generate segmentation\n",
        "            outputs = model(image.unsqueeze(0), mode='segment')\n",
        "            seg_pred = outputs['segmentation'].squeeze(0).argmax(0).cpu()\n",
        "\n",
        "            print(f\"\\nSample {i+1}:\")\n",
        "            print(f\"Generated Caption: {generated_caption}\")\n",
        "            print(f\"Segmentation shape: {seg_pred.shape}\")\n"
      ],
      "metadata": {
        "id": "pyNnZkhQBsTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Main Execution Function\n"
      ],
      "metadata": {
        "id": "v8jPjbPPCaPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Starting complete training pipeline...\")\n",
        "\n",
        "    # Setup from your existing code\n",
        "    setup_dict = setup_training()  # Your existing function\n",
        "    vocab = setup_dict['vocab']\n",
        "    idx2word = setup_dict['idx2word']\n",
        "    model = setup_dict['integrated_model']\n",
        "    device = setup_dict['device']\n",
        "\n",
        "    # Create data loaders\n",
        "    print(\"Creating data loaders...\")\n",
        "    train_loader, val_loader = create_data_loaders(vocab, batch_size=16)\n",
        "\n",
        "    # Start training\n",
        "    print(\"Starting training...\")\n",
        "    train_losses, val_losses = train_model_with_logging(\n",
        "        model, train_loader, val_loader, vocab, idx2word,\n",
        "        num_epochs=5, device=device\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    bleu, iou = calculate_metrics(model, val_loader, vocab, idx2word, device)\n",
        "\n",
        "    print(\"\\nTraining completed successfully!\")\n",
        "    return model, train_losses, val_losses\n"
      ],
      "metadata": {
        "id": "mKC-Va5lBvZV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    \"\"\"Instructions for downloading COCO dataset\"\"\"\n",
        "    instructions = \"\"\"\n",
        "    To use real COCO dataset, follow these steps:\n",
        "    \n",
        "    1. Create a directory structure:\n",
        "       coco/\n",
        "       ├── train2017/\n",
        "       ├── val2017/\n",
        "       └── annotations/\n",
        "           ├── instances_train2017.json\n",
        "           ├── instances_val2017.json\n",
        "           ├── captions_train2017.json\n",
        "           └── captions_val2017.json\n",
        "    \n",
        "    2. Download from: https://cocodataset.org/#download\n",
        "       - 2017 Train images [118K/18GB]\n",
        "       - 2017 Val images [5K/1GB]\n",
        "       - 2017 Train/Val annotations [241MB]\n",
        "    \n",
        "    3. Install pycocotools:\n",
        "       pip install pycocotools\n",
        "    \n",
        "    4. Update the dataset paths in create_data_loaders()\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "pMFdIlX7B8pr"
      }
    }
  ]
}